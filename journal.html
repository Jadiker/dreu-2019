<!DOCTYPE html>
<html>
    <head>
        <link href="nav_bar.css" rel="stylesheet">
        <link href="page.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
        <title>
            London DREU Website
        </title>
        <ul id="navbar">
            <li><a href="index.html">Homepage</a></li>
            <li><a href="about_me.html">About Me</a></li>
            <li><a href="about_mentor.html">About My Mentor</a></li>
            <li><a href="journal.html">Journal</a></li>
            <li><a href="project.html">Projects</a></li>
            <li><a href="final_report.html">Final Report</a></li>
        </ul>
    </head>
    <body id="page">
        <h1>Journal</h1>
        <h2 class="journal-week">Week 1 (May 27th)</h2>
        <p>This week, I made my first attempt of determining what project I wanted to work on. My initial idea was based off of a technique I had seen used in the training of an OpenAI robot hand that <a href="https://arxiv.org/pdf/1808.00177.pdf" target="_blank">learned to turn a cube</a> via reinforcement learning inside of vitual reality. One of the key insights into how to get a system like this to work, is that they used domain randomization. This means that many of the colors and positions and other physical properties of things within the simulation were randomized between trials.</p>
        <img src="openai_domain_randomization.jpg" style="width:400px;height:auto;">

        <p>This forced the AI mechanisms they were using to generalize over a range of values, which helped them to be more robust in the real world. I wanted to explore ideas to see if I could come up with a process that would create a domain randomizer for any domain. That is, instead of OpenAI researchers determining what properties should be changed and within what ranges, they should be able to feed my program video feeds of the real situation, and the program would be able to determine properties that should be changed and within what range in order to have the best overall simulator for training the hand controller.</p>
        <p>This idea led me straight into researching adversarial attacks on neural networks. Why? Well, adversarial attacks are done by <i>very slightly</i> changing an input image in order to trick a neural network into picking a different answer. (<a href="https://qr.ae/TWnY10" target="_blank">This Quora answer</a> gives a decent overview of this phenomena.) So, any neural network that could deal with adversarial attacks likely had some sort of internal model as to how the input could change in ways that should not affect the output. This is essentially all that domain randomization is; determining how to change the input in ways that should not affect the output.</p>
        <p>I debated researching this further, but during my research on adversarial attacks, I was brought back to the area of AI interpretability. (I attempted some work with AI interpretability <a href="https://jadiker.github.io/dreu-2018/projects.html" target="_blank">last year</a>, trying to see if capsule networks used concepts similar to humans.) This year, I decided I really wanted to understand what it meant for AIs to be interpretable. This is especially important to me since I'm concentrating in both computer science and philosophy, so this is a question topic that comes up a fair amount.</p>
        <p>One of the <a href="https://arxiv.org/abs/1806.00069" target="_blank">papers</a> I read made a distinction that I think is very important: persuasive systems versus transparent systems. In a <a href="https://arxiv.org/abs/1901.03729" target="_blank">different paper</a> I read about last year, a researcher created an AI that used one system to make actions, and a completely different system that came up with justifications for those actions based off of the justifications humans had used when taking those same actions. I think this type of action explanation is awful, because it's persuasive instead of transparent. That is, the computer is merely giving you an explanation that looks plausible based off of the explanations <i>humans</i> have given previously. How the computer decides to make decisions in this case is not the same method that humans use to make decisions. So giving an explanation that is unrelated to how the AI functions I think is an extremely dangerous method, and I wish the authors had been more clear about the possible downsides of using their proposal.</p>
        <p>I ended the week thinking that my main research was likely going to be in trying to come up with a new method (or at least a better definition) for interpretability in artifical intelligence.</p>
        <h2 class="journal-week">Week 2 (June 3rd)</h2>
        <p>My goal for this week was to understand two things:</p>
        <ol>
            <li>What the current definitions for interpretability in AI are</li>
            <li>What my definition of interpretability in AI is</li>
        </ol>
        <p>One of the best papers I was able to find was <a href="https://link.springer.com/chapter/10.1007%2F978-94-015-9731-9_2" target="_blank"><i>What Good is an Explanation?</i></a> by Peter Lipton. It was fairly old (2001), but very helpful in terms of understanding different philosophical explanations of understanding. I ended up giving a small presentation to my research group about the different models he proposes. Overall, I think the paper is helpful, but I don't quite agree with it. His main method was using three particular facts about explanations in order to show that certain definitions of an explanation either do or don't allow those two facts to be true. As a general principle, I think this approach works alright, but I think the facts he picked were odd, and not completely representative of what an explanation is (to me, at least). The facts were:</p>
        <ol>
            <li>There's a difference between knowledge and understanding (Gap Between Knowledge and Understanding)</li>
            <li>An explanation doesnâ€™t require an explanation for itself (Why Regress)</li>
            <li>An explanation of a statement can be partially justified by the statement itself (Self-evidencing Explanations)</li>
        </ol>
        <p>He gives a case that I think will help to explain these three facts. In this case, there is a ripped up stuffed teddy bear on the floor. We'd like an explanation as to why there is a ripped up stuffed teddy bear on the floor. In this case, we <i>know</i> that there is a ripped up stuffed teddy bear on the floor, but we don't <i>understand</i> why it is there. Also, an explanation may be that our pet dog ripped it up. This explanation works, even though we may not understand <i>why</i> the dog ripped it up. That is, the explanation still does a good job of explaining, without needing further explanation. Finally, part of the reason why we might think that the dog ripped up the stuffed teddy bear is because there is a stuffed teddy bear on the floor.</p>
        <p>While I think these facts are true of some explanations, I'm not sure if they're true for all explanations, and so I think it's difficult to find good theories based on these explanations.</p>
        <p>After reading his introduction and main argument, I ended up thinking that all of the theories he mentioned were properties of a good explanation. I won't detail my thoughts here, but I don't think the one account he chose ("Causality") is necessarily all that an explanation is; I think that each of the accounts he mentions is a property of explanations, and the more of those properties an explanation satisfies, the more likely we are to consider it to be a good explanation.</p>
        <p>Overall, this helped me a lot with understanding how different people might categorize and understand explanations and AI interpretability.</p>
        <br/>
        <p>Throughout the week, I also developed and tested my own definitions of AI interpretability. The one that seemed to make the most sense, and was the most stable, was Predictability. That is, in order for an AI to be interpretable, it must be predictable.</p>
        <p>The example that I came up with isn't the best, but it at least gets at the idea. If there's a house, and we have ways of running inspections on the house to determine if it's safe to be in or not, then the house is predictable. That is, if the house passes the inspection, we know that it's safe to go in the house. However, if there is no inspection proccess, or houses that have passed inspection break without warning, then the house is not predictable.</p>
        <p>A similar thing goes for AI. If the AI gives us explanations (or we "inspect" it to determine explanations for its behavior), then those explanations should allow us to predict how the AI will behave. To the extent that the explanations don't allow us to make accurate predictions about AI, I don't believe the AI should be considered "interpretable" or "safe" to use in high-stakes scenarios.</p>
        <p>For example, we have yet to find very good ways to make neural networks interpretable. Why? Because in order to determine how the network will respond to a particular input, we currently have to send the vector through the network. That is, the best way to predict how the network will react is to see how it actually reacts. From the research I've done, we haven't found a good way yet of generalizing how neural networks will respond in particular cases. They're undpredictable. Thus, instances of adversarial attacks on networks still surprise us; they are still unsafe.</p>
        
    </body>
</html>
