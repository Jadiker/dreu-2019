<!DOCTYPE html>
<html>
    <head>
        <div data-include="common_header.htmlt.html"></div>
    </head>
    <body id="page">
        <h1>Journal</h1>
        <h2 class="journal-week">Week 1 (May 27th)</h2>
        <p>This week, I made my first attempt of determining what project I wanted to work on. My initial idea was based off of a technique I had seen used in the training of an OpenAI robot hand that [learned to turn a cube](https://arxiv.org/pdf/1808.00177.pdf) via reinforcement learning inside of vitual reality. One of the key insights into how to get a system like this to work, is that they used domain randomization. This means that many of the colors and positions and other physical properties of things within the simulation were randomized between trials.</p>
        <img src="openai_domain_randomization.jpg" style="width:400px;height:auto;">

        <p>This forced the AI mechanisms they were using to generalize over a range of values, which helped them to be more robust in the real world. I wanted to explore ideas to see if I could come up with a process that would create a domain randomizer for any domain. That is, instead of OpenAI researchers determining what properties should be changed and within what ranges, they should be able to feed my program video feeds of the real situation, and the program would be able to determine properties that should be changed and within what range in order to have the best overall simulator for training the hand controller.</p>
        <p>This idea led me straight into researching adversarial attacks on neural networks. Why? Well, adversarial attacks are done by <i>very slightly</i> changing an input image in order to trick a neural network into picking a different answer. ([This Quora answer](https://qr.ae/TWnY10) gives a decent overview of this phenomena.) So, any neural network that could deal with adversarial attacks likely had some sort of internal model as to how the input could change in ways that should not affect the output. This is essentially all that domain randomization is; determining how to change the input in ways that should not affect the output.</p>
        <p>I debated researching this further, but during my research on adversarial attacks, I was brought back to the area of AI interpretability. (I attempted some work with AI interpretability [last year](https://jadiker.github.io/dreu-2018/projects.html), trying to see if capsule networks used concepts similar to humans.) This year, I decided I really wanted to understand what it meant for AIs to be interpretable. This is especially important to me since I'm concentrating in both computer science and philosophy, so this is a question topic that comes up a fair amount.</p>
        <p>One of the [papers](https://arxiv.org/abs/1806.00069) I read made a distinction that I think is very important: persuasive systems versus transparent systems. In a [different paper](https://arxiv.org/abs/1901.03729) I read about last year, a researcher created an AI that used one system to make actions, and a completely different system that came up with justifications for those actions based off of the justifications humans had used when taking those same actions. I think this type of action explanation is awful, because it's persuasive instead of transparent. That is, the computer is merely giving you an explanation that looks plausible based off of the explanations <i>humans</i> have given previously. How the computer decides to make decisions in this case is not the same method that humans use to make decisions. So giving an explanation that is unrelated to how the AI functions I think is an extremely dangerous method, and I wish the authors had been more clear about the possible downsides of using their proposal.</p>
        <p>I ended the week thinking that my main research was likely going to be in trying to come up with a new method (or at least a better definition) for interpretability in artifical intelligence.</p>
        <h2 class="journal-week">Week 2 (June 3rd)</h2>
        <p>My goal for this week was to understand two things:</p>
        <ol>
            <li>What the current definitions for interpretability in AI are</li>
            <li>What my definition of interpretability in AI is</li>
        </ol>
        <p>One of the best papers I was able to find was [<i>What Good is an Explanation?</i>](https://link.springer.com/chapter/10.1007%2F978-94-015-9731-9_2) by Peter Lipton. It was fairly old (2001), but very helpful in terms of understanding different philosophical explanations of understanding. I ended up giving a small presentation to my research group about the different models he proposes. Overall, I think the paper is helpful, but I don't quite agree with it. His main method was using three particular facts about explanations in order to show that certain definitions of an explanation either do or don't allow those three facts to be true. As a general principle, I think this approach works alright, but I think the facts he picked were odd, and not completely representative of what an explanation is (to me, at least). The facts were:</p>
        <ol>
            <li>There's a difference between knowledge and understanding (Gap Between Knowledge and Understanding)</li>
            <li>An explanation doesnâ€™t require an explanation for itself (Why Regress)</li>
            <li>An explanation of a statement can be partially justified by the statement itself (Self-evidencing Explanations)</li>
        </ol>
        <p>He gives a case that I think will help to explain these three facts. In this case, there is a ripped up stuffed teddy bear on the floor. We'd like an explanation as to why there is a ripped up stuffed teddy bear on the floor. In this case, we <i>know</i> that there is a ripped up stuffed teddy bear on the floor, but we don't <i>understand</i> why it is there. Also, an explanation may be that our pet dog ripped it up. This explanation works, even though we may not understand <i>why</i> the dog ripped it up. That is, the explanation still does a good job of explaining, without needing further explanation. Finally, part of the reason why we might think that the dog ripped up the stuffed teddy bear is because there is a stuffed teddy bear on the floor. That is, part of the <i>evidence</i> that the dog ripped up the stuffed teddy bear is the fact that the teddy bear is ripped up.</p>
        <p>While I think these facts are true of some explanations, I'm not sure if they're true for all explanations, and so I think it's difficult to find good theories based on these explanations.</p>
        <p>After reading his introduction and main argument, I ended up thinking that all of the theories he mentioned were properties of a good explanation. I won't detail my thoughts here, but I don't think the one account he chose ("Causality") is necessarily all that an explanation is; I think that each of the accounts he mentions is a property of explanations, and the more of those properties an explanation satisfies, the more likely we are to consider it to be a good explanation.</p>
        <p>Overall, this helped me a lot with understanding how different people might categorize and understand explanations and AI interpretability.</p>
        <br/>
        <p>Throughout the week, I also developed and tested my own definitions of AI interpretability. The one that seemed to make the most sense, and was the most stable, was Predictability. That is, in order for an AI to be interpretable, it must be predictable.</p>
        <p>The analogy that I came up with isn't the best, but it at least gets at the idea. If there's a house, and we have ways of running inspections on the house to determine if it's safe to be in or not, then the house is predictable. That is, if the house passes the inspection, we know that it's safe to go in the house. However, if there is no inspection proccess, or houses that have passed inspection break without warning, then the house is not predictable.</p>
        <p>A similar thing goes for AI. If the AI gives us explanations (or we "inspect" it to determine explanations for its behavior), then those explanations should allow us to predict how the AI will behave. To the extent that the explanations don't allow us to make accurate predictions about AI, I don't believe the AI should be considered "interpretable" or "safe" to use in high-stakes scenarios.</p>
        <p>For example, we have yet to find very good ways to make neural networks interpretable. Why? Because in order to determine how the network will respond to a particular input, we currently have to send the vector through the network. That is, the best way to predict how the network will react is to see how it actually reacts. From the research I've done, we haven't found a good way yet of generalizing how neural networks will respond in particular cases. They're undpredictable. Thus, instances of adversarial attacks on networks still surprise us; neural networks are still unsafe.</p>
        <h2 class="journal-week">Week 3 (June 10th)</h2>
        <p>Last week, in the middle of doing my own philosophical research, I also took a short break to look at some videos from the [OpenAI Robotics Symposium](https://www.youtube.com/watch?v=WRsxoVB8Yng) on robotics. One of the [talks on self-supervised learning](https://www.youtube.com/watch?v=WRsxoVB8Yng&t=7948s) really caught my interest. In the talk, they discuss how they were able to use a time-contrastive network (TCN) in order to teach a robot how to pour liquid into a cup, just from having it watch videos of people pouring liquid into a cup. I was very interested in how they did this, so I read the paper and more of the author's work to understand it.</p>
        <p>I was especially interested if TCNs could be used in style transfer. Style transfer is an area that intrigues me due to my interests mentioned above about domain randomization and robust understanding (leading to predictability/interpretability). The idea is that if a computer can create the same product but in a different style, it likely understands what the product is. Essentially, domain randomization is just changing the <i>style</i> of the environment, without affecting core aspects of the environment itself. So if TCNs could be used for style transfer, they could likely be useful in these other problems as well.</p>
        <p>At this point, I realized that I would be very interested in seeing if I could take the time-contrastive network from [the paper](https://arxiv.org/abs/1704.06888) and apply it to music. Instead of watching multiple people pour liquid into cups from multiple angles, and learning to imitate them, it would instead listen to the same song played with multiple different instruments, and then learn to output MIDI music with new instruments. This is the main idea behind my project for the summer.</p>
        <p>So, as my mentor recommended, the first thing to do was to see if I could get [Google's TCN](https://github.com/tensorflow/models/tree/master/research/tcn) up and running. I tried installing the software both in an Ubuntu virtual machine and straight on my Mac, but they both failed. Running on the Ubuntu VM didn't work because I didn't have a GPU, and running on my Mac didn't work because my Mac doesn't support the AVX instructions that the newer versions of Tensorflow use. (I have an old 17'' Mac.) And that was as far as I got this week.</p>
        <h2 class="journal-week">Week 4 (June 17th)</h2>
        <p>After trying both of my personal computer options, it was time to move on to [MSI](https://www.msi.umn.edu/) and see if I could get the Google TCN up and running on there. I had to fight my way through a few bugs and re-accumulate to the supercomputing environment. (I had done some work for a graduate student [last year](https://jadiker.github.io/dreu-2018/projects.html) on MSI.) But, in the end, I was able to get the tests up and running and passing.</p>
        <p>However, after spending some time looking at the codebase and realizing that it would take me weeks to modify it in a way that might potentially be able to help me, I decided to look into implementing the paper (or at least the main ideas behind it) on my own. (Their implementation made a pretty ubiquitous assumption that the data you were using was video data, and it was going to take a while to figure out all the places where switching to audio would break their code and program workflow.)</p>
        <p>So, I spent some time looking at recent advances both in using the triplet loss function (which is what the TCN uses to train) and style transfer. Over this period, I learned a lot about how the triplet loss function works, and about how it's normally used for [face recognition](https://arxiv.org/abs/1503.03832).</p>
        <p>I also ran into some very interesting audio style transfer projects. The best one was Facebook's AI Research's (FAIR) [Universal Music Translation Network](https://research.fb.com/publications/a-universal-music-translation-network/) (UMTN). This does almost exactly what I want to do with TCNs (and it does it very well), but it edits the audio files directly. This is cool, but not exactly what I want. I want the computer to be able to write music similar to how humans do, by manipulating the notes, rather than the audio frequencies themselves. Since the UMTN edits the audio directly, it also ends up sounding a lot less clear than a recording of an actual instrument being played. They also mention this issue in [the paper itself](https://arxiv.org/abs/1805.07848), as the results of one of their experiments show that the audio quality is nowhere near as good as an actual instrument. Using a MIDI output would hopefully get around this issue.</p>
        <p>Another interesting find was the [Google Magenta](https://magenta.tensorflow.org/) project, which worked more on the side of instrument blending, music structure, and audio transcription. Their [piano to MIDI](https://piano-scribe.glitch.me/) software is actually the best MIDI converter I've seen available online for free. I also like that they're doing the machine learning locally on the computer, so that they don't have to host the service - it makes it more likely that they'll leave it up and running since their costs will be minimal. (However, it does create a performance gap for people with computers that aren't very fast. A high-end gaming computer takes a second to do what may take another computer multiple minutes.)</p>
        <br/>
        <p>Part of the workflow of the complete self-supervised imitation process is that a reinforcement learning algorithm uses the encodings learned from the TCN as a signal to judge its own performance. In my case, the AI would need to have some way of outputting MIDI notes which could be played, turning it into audio. Then, this audio would be passed through the TCN in order to get an encoding. This encoding is then measured against the encoding of the song that the AI is trying to imitate. The more different the MIDI encoding is from the song's encoding, the lower the reward the reinforcement learning part of the AI recieves.</p>
        <p>So, I decided to build a system that would allow an AI to output a vector and convert it into MIDI. I wanted it to be similar to how humans play instruments, so I decided that I wanted each number in the vector to symbolize a note. The AI could press down on a note by giving that note a value of .5 or higher (up to 1). The higher the value, the harder the note is pressed. Then, after a note is pressed, a value of .5 or higher again will release the note. This seemed to be similar enough to how humans play piano. There's no sustain pedal in this model, but the AI still has to determine when to press notes and when to release them. Each vector represented one beat of the song. How fast a beat is can be changed to allow the AI to have more precision.</p>
        <p>I was able to get this system up and working, but did not find a suitable dataset to test to see if I could get a reinforcement learning algorithm to actually use it. It was hard to find datasets where I could convert either MIDI or audio into "press-and-release"-type vectors. I'm sure if I spent more time studying the MIDI format, I could have figured out how to reverse engineer the data, since I already found [a program](https://pypi.org/project/MIDIUtil/) that forward-engineered it. The closest I got was being able to take audio files from a [MIREX dataset](http://www.ee.columbia.edu/~graham/mirex_melody/) which had the melody pitch annotated, and convert that into MIDI, so that the MIDI file could play the piano to exactly match the pitch of the main melody of the song. This, in turn, could be converted into the "press and release"-type vectors, if need be. However, this obviously wasn't what I wanted, because it sounded very forced - the piano would jump from note to note every hundreth of a second, trying to match the singer's pitch. What I wanted was MIDI files that sound like what a human would come up with if asked to transcribe a song.</p>
        <p> For example, here is what the original song sounds like: </p>
        <audio controls>
          <source src="mirex_wav.wav" type="audio/mpeg">
              Your browser does not support the audio element.
        </audio>
        <p> And this is what the MIDI transcript sounds like: </p>
        <audio controls>
          <source src="mirex_midi.mp3" type="audio/mpeg">
              Your browser does not support the audio element.
        </audio>
        <p>Not ideal. However, it's a step in the right direction.</p>
        <p>This week basically ended with me having a working program that could convert the MIREX files into vectors, and convert those vectors into MIDI files that could then be played.</p>
        <!-- <h2 class="journal-week">Week 5 (June 24th)</h2> -->
    </body>
</html>
