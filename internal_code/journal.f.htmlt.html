<!DOCTYPE html>
<html>
    <head>
        <div data-include="common_header.htmlt.html"></div>
    </head>
    <body id="page">
        <h1>Journal</h1>
        <h2 class="journal-week">Week 1 (May 27th)</h2>
        <p>This week, I made my first attempt of determining what project I wanted to work on. My initial idea was based off of a technique I had seen used in the training of an OpenAI robot hand that [learned to turn a cube](https://arxiv.org/pdf/1808.00177.pdf) via reinforcement learning inside of vitual reality. One of the key insights into how to get a system like this to work, is that they used domain randomization. This means that many of the colors and positions and other physical properties of things within the simulation were randomized between trials.</p>
        <img src="openai_domain_randomization.jpg" style="width:400px;height:auto;">

        <p>This forced the AI mechanisms they were using to generalize over a range of values, which helped them to be more robust in the real world. I wanted to explore ideas to see if I could come up with a process that would create a domain randomizer for any domain. That is, instead of OpenAI researchers determining what properties should be changed and within what ranges, they should be able to feed my program video feeds of the real situation, and the program would be able to determine properties that should be changed and within what range in order to have the best overall simulator for training the hand controller.</p>
        <p>This idea led me straight into researching adversarial attacks on neural networks. Why? Well, adversarial attacks are done by <i>very slightly</i> changing an input image in order to trick a neural network into picking a different answer. ([This Quora answer](https://qr.ae/TWnY10) gives a decent overview of this phenomena.) So, any neural network that could deal with adversarial attacks likely had some sort of internal model as to how the input could change in ways that should not affect the output. This is essentially all that domain randomization is; determining how to change the input in ways that should not affect the output.</p>
        <p>I debated researching this further, but during my research on adversarial attacks, I was brought back to the area of AI interpretability. (I attempted some work with AI interpretability [last year](https://jadiker.github.io/dreu-2018/projects.html), trying to see if capsule networks used concepts similar to humans.) This year, I decided I really wanted to understand what it meant for AIs to be interpretable. This is especially important to me since I'm concentrating in both computer science and philosophy, so this is a question topic that comes up a fair amount.</p>
        <p>One of the [papers](https://arxiv.org/abs/1806.00069) I read made a distinction that I think is very important: persuasive systems versus transparent systems. In a [different paper](https://arxiv.org/abs/1901.03729) I read about last year, a researcher created an AI that used one system to make actions, and a completely different system that came up with justifications for those actions based off of the justifications humans had used when taking those same actions. I think this type of action explanation is awful, because it's persuasive instead of transparent. That is, the computer is merely giving you an explanation that looks plausible based off of the explanations <i>humans</i> have given previously. How the computer decides to make decisions in this case is not the same method that humans use to make decisions. So giving an explanation that is unrelated to how the AI functions I think is an extremely dangerous method, and I wish the authors had been more clear about the possible downsides of using their proposal.</p>
        <p>I ended the week thinking that my main research was likely going to be in trying to come up with a new method (or at least a better definition) for interpretability in artifical intelligence.</p>
        <h2 class="journal-week">Week 2 (June 3rd)</h2>
        
    </body>
</html>
